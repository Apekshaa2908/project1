# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CmAZ6Dxz3JYVIvFyfUU_4_4pfq5-zt_
"""

# app.py
# Full Streamlit app: News Events â€” Data Quality Pipeline (single-file)
# - Paste over your existing app.py
# - Implements:
#   * Steps 1..10 (parsing, profiling, missingness, date cleaning, outliers, normality, dedupe, drop body/title, EDA)
#   * BEFORE vs AFTER preprocessing comparison (shape + missing % per-table)
#   * Only two downloadable cleaned CSVs (df_data, df_included)
#   * Automatically DROP & CREATE Redshift tables, PUSH cleaned data, create a VIEW with all columns (for Tableau)
#   * DQ metrics summary (Completeness, Uniqueness, Accuracy, Consistency, Timeliness)
#   * Dynamic SNS threshold (sidebar control); publish alert if combined DQ < threshold
#   * Streamlit logs shown in-app
#
# IMPORTANT:
# - Ensure your Streamlit environment has: boto3, botocore, pandas, numpy, matplotlib, seaborn, psycopg2
# - Redshift inserts here use psycopg2 INSERT (suitable for moderate-sized uploads). For large scale, switch to S3+COPY.
# - Change any constants near the CONFIG section if your infra differs.

import streamlit as st
import pandas as pd
import numpy as np
import json
import io
import boto3
import botocore
import matplotlib.pyplot as plt
import seaborn as sns
import psycopg2
import logging
from datetime import datetime, timedelta
from typing import Tuple
from dateutil.relativedelta import relativedelta

# -------------------------
# PAGE & BASIC CONFIG
# -------------------------
st.set_page_config(page_title="News Events â€” Data Quality Pipeline", layout="wide")
st.title("ðŸ§¹ News Events â€” Data Quality Pipeline")

# -------------------------
# USER CONFIG - change if needed
# -------------------------
# Secrets / buckets / redshift / sns
SECRET_ARN = "arn:aws:secretsmanager:ap-southeast-2:113693925773:secret:StreamlitSecret-3HqZLU"
RAW_BUCKET = "news-events-raw-data"
CLEANED_BUCKET = "news-events-cleaned-data"
OUTPUT_DATA_KEY = "cleaned_df_data.csv"
OUTPUT_INCLUDED_KEY = "cleaned_df_included.csv"

REDSHIFT_CONFIG = {
    "host": "news-events-cluster.c1kcr1ovplqi.ap-southeast-2.redshift.amazonaws.com",
    "port": 5439,
    "dbname": "news_events_db",
    "user": "awsuser",
    "password": "YUDHRsebd889%$",
}

SNS_TOPIC_ARN = "arn:aws:sns:ap-southeast-2:113693925773:NewsEventsDQAlerts"
AWS_REGION = "ap-southeast-2"

# -------------------------
# LOGGING (in-app buffer)
# -------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("news_events_dq")
_log_lines = []

def log(msg: str):
    ts = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    _log_lines.append(f"{ts} - {msg}")
    logger.info(msg)

# -------------------------
# Helper functions
# -------------------------
def read_jsonl_bytes(file_bytes: bytes) -> list:
    text = file_bytes.decode("utf-8", errors="replace")
    records = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        try:
            records.append(json.loads(s))
        except Exception:
            # skip malformed lines
            continue
    return records

def parse_records_to_dfs(records_list: list) -> Tuple[pd.DataFrame, pd.DataFrame]:
    data_rows = []
    included_rows = []
    for rec in records_list:
        if not isinstance(rec, dict):
            continue
        for item in rec.get("data", []) or []:
            attributes = item.get("attributes", {}) or {}
            relationships = item.get("relationships", {}) or {}
            data_rows.append({
                "event_id": item.get("id"),
                "summary": attributes.get("summary"),
                "body": attributes.get("body"),
                "title": attributes.get("title"),
                "category": attributes.get("category"),
                "found_at": attributes.get("found_at"),
                "confidence": attributes.get("confidence"),
                "article_sentence": attributes.get("article_sentence"),
                "location": attributes.get("location"),
                "award": attributes.get("award"),
                "effective_date": attributes.get("effective_date"),
                "product": attributes.get("product"),
                "company_id": (relationships.get("company1") or {}).get("data", {}).get("id"),
                "source_id": (relationships.get("most_relevant_source") or {}).get("data", {}).get("id"),
            })
        for item in rec.get("included", []) or []:
            attrs = item.get("attributes", {}) or {}
            flat = {"included_id": item.get("id"), "type": item.get("type")}
            for k, v in attrs.items():
                flat[k] = v
            included_rows.append(flat)
    df_data = pd.DataFrame(data_rows)
    df_included = pd.DataFrame(included_rows)
    return df_data, df_included

def basic_profile_df(df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    n = len(df)
    for col in df.columns:
        s = df[col]
        rows.append({
            "column": col,
            "dtype": str(s.dtype),
            "non_null_count": int(s.notnull().sum()),
            "null_count": int(s.isnull().sum()),
            "pct_missing": round(float(s.isnull().sum() / n * 100) if n else 0.0, 2),
            "unique_values": int(s.nunique(dropna=True))
        })
    return pd.DataFrame(rows)

def detect_outliers_iqr(series: pd.Series):
    ser = series.dropna().astype(float)
    if ser.empty:
        return pd.Series([False]*len(series), index=series.index), {}
    q1 = ser.quantile(0.25)
    q3 = ser.quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    mask = (series < lower) | (series > upper)
    return mask.fillna(False), {"q1": float(q1), "q3": float(q3), "lower": float(lower), "upper": float(upper)}

def make_tableau_friendly_date_str(val, want_date_only=False):
    # Conservative parser -> return empty string when cannot parse
    if val is None:
        return ""
    if isinstance(val, (float, int)) and np.isnan(val):
        return ""
    s = str(val).strip()
    if s == "" or s.lower() in ("nan", "none", "nat"):
        return ""
    try:
        dt = pd.to_datetime(s, errors="coerce")
        if pd.isna(dt):
            # Try to keep the first 10 chars if looks like YYYY-MM-DD
            if len(s) >= 10 and s[:4].isdigit():
                return s[:10] if want_date_only else (s[:10] + " 00:00:00")
            return ""
        if want_date_only:
            return dt.strftime("%Y-%m-%d")
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return ""

def nulls_to_blanks(df: pd.DataFrame):
    for col in df.columns:
        if df[col].dtype == "object":
            df[col] = df[col].where(df[col].notnull(), "")
        else:
            df[col] = df[col].apply(lambda x: "" if (pd.isna(x) or x is None or (isinstance(x, float) and np.isnan(x))) else x)
    return df

def truncate_all_columns(df: pd.DataFrame, max_len=65000):
    for col in df.columns:
        df[col] = df[col].astype(str).apply(lambda x: x[:max_len] if isinstance(x, str) else x)
    return df

# -------------------------
# S3 connection via Secrets Manager (existing logic)
# -------------------------
def connect_s3_via_secret(secret_arn: str):
    sm = boto3.client("secretsmanager", region_name=AWS_REGION)
    resp = sm.get_secret_value(SecretId=secret_arn)
    secret = json.loads(resp["SecretString"])
    region = secret.get("region", AWS_REGION)
    session = boto3.session.Session(
        aws_access_key_id=secret.get("aws_access_key_id"),
        aws_secret_access_key=secret.get("aws_secret_access_key"),
        region_name=region
    )
    return session.client("s3", region_name=region)

# -------------------------
# AWS & UI: sidebar config + dynamic SNS threshold
# -------------------------
st.sidebar.header("AWS + App configuration")
use_secret = st.sidebar.checkbox("Use Secrets Manager for S3 (recommended)", value=True)
s3_client = None
if use_secret:
    try:
        s3_client = connect_s3_via_secret(SECRET_ARN)
        st.sidebar.success("Connected to S3 via Secrets Manager")
        log("Connected to S3 via Secrets Manager")
    except Exception as e:
        st.sidebar.error(f"Secrets Manager connection failed: {e}")
        log(f"Secrets Manager connection failed: {e}")

if s3_client is None:
    st.sidebar.info("Fallback: Streamlit will use default boto3 client (env creds).")
    s3_client = boto3.client("s3", region_name=AWS_REGION)

# Dynamic SNS threshold
st.sidebar.markdown("### DQ Alert threshold (dynamic)")
dq_threshold = st.sidebar.slider("Send SNS alert when overall combined DQ (%) falls below:", min_value=0, max_value=100, value=60, step=1)
st.sidebar.write("Current threshold:", dq_threshold, "%")

# -------------------------
# STEP 1 - File upload & parsing
# -------------------------
st.header("1ï¸âƒ£ Step 1 â€” Upload & parse JSONL files into df_data and df_included")
st.markdown("Upload one or more .jsonl files (each top-level JSON object should have `data` and/or `included`).")
uploaded_files = st.file_uploader("Upload JSONL files", type=["jsonl", "json"], accept_multiple_files=True)

if not uploaded_files:
    st.info("Please upload at least one JSONL file to begin (or re-run the page after uploading).")
    st.stop()

merge_with_s3 = st.sidebar.checkbox("Merge with existing cleaned file in S3 (if exists)", value=True)
drop_body_choice = st.sidebar.checkbox("Drop 'body' column from df_data (recommended)", value=True)

# accumulate "before" snapshots for BEFORE vs AFTER preprocessing
before_snapshot = {}

all_records = []
for f in uploaded_files:
    try:
        raw_bytes = f.getvalue()
        # upload raw file to raw bucket (best-effort)
        try:
            s3_client.put_object(Bucket=RAW_BUCKET, Key=f.name, Body=raw_bytes)
            log(f"Uploaded raw file '{f.name}' to {RAW_BUCKET}")
        except Exception as e:
            log(f"Raw upload skipped/failed for {f.name}: {e}")
        recs = read_jsonl_bytes(raw_bytes)
        all_records.extend(recs)
    except Exception as e:
        st.error(f"Failed to read {f.name}: {e}")
        log(f"Failed to read {f.name}: {e}")

df_data, df_included = parse_records_to_dfs(all_records)
st.success(f"Parsed files â†’ df_data rows: {len(df_data)} | df_included rows: {len(df_included)}")
log(f"Parsing complete: df_data={len(df_data)}, df_included={len(df_included)}")

# Snapshot BEFORE preprocessing (shape + missing%)
before_snapshot['df_data_shape'] = df_data.shape
before_snapshot['df_included_shape'] = df_included.shape
before_snapshot['df_data_missing_pct'] = round(df_data.isnull().mean().mean() * 100, 2) if len(df_data) else 0.0
before_snapshot['df_included_missing_pct'] = round(df_included.isnull().mean().mean() * 100, 2) if len(df_included) else 0.0
before_snapshot['df_data_profile'] = basic_profile_df(df_data)
before_snapshot['df_included_profile'] = basic_profile_df(df_included)

# -------------------------
# STEP 2 - Basic info (df.info-like) + column dictionary
# -------------------------
st.header("2ï¸âƒ£ Step 2 â€” Basic info & column dictionary")
col1, col2 = st.columns(2)
with col1:
    st.subheader("ðŸ“˜ df_data â€” preview & profile")
    st.dataframe(df_data.head(10), use_container_width=True)
    info_df = basic_profile_df(df_data)
    st.write("Shape:", df_data.shape)
    st.subheader("ðŸ“Š df_data â€” Basic Information")
    st.dataframe(info_df, use_container_width=True)
    st.markdown("#### ðŸ§© Column Dictionary â€” df_data")
    st.dataframe(pd.DataFrame([
        {"Column Name":"event_id","Data Type":"string","Description":"Unique identifier for each event"},
        {"Column Name":"summary","Data Type":"string","Description":"Headline/summary text"},
        {"Column Name":"category","Data Type":"string","Description":"Event classification"},
        {"Column Name":"found_at","Data Type":"string","Description":"Detected timestamp (raw)"},
        {"Column Name":"confidence","Data Type":"float","Description":"Confidence score 0-1"},
        {"Column Name":"article_sentence","Data Type":"string","Description":"Sentence extracted"},
        {"Column Name":"location","Data Type":"string","Description":"Event location"},
        {"Column Name":"award","Data Type":"string","Description":"Award mentioned"},
        {"Column Name":"effective_date","Data Type":"string","Description":"Date-only when event was effective"},
        {"Column Name":"product","Data Type":"string","Description":"Product referenced"},
        {"Column Name":"company_id","Data Type":"string","Description":"FK to included.included_id (company)"},
        {"Column Name":"source_id","Data Type":"string","Description":"FK to included.included_id (source)"},
    ]), use_container_width=True)
with col2:
    st.subheader("ðŸ“™ df_included â€” preview & profile")
    st.dataframe(df_included.head(10), use_container_width=True)
    info_inc = basic_profile_df(df_included)
    st.write("Shape:", df_included.shape)
    st.subheader("ðŸ“Š df_included â€” Basic Information")
    st.dataframe(info_inc, use_container_width=True)
    st.markdown("#### ðŸ§© Column Dictionary â€” df_included")
    st.dataframe(pd.DataFrame([
        {"Column Name":"included_id","Data Type":"string","Description":"Identifier for included entity"},
        {"Column Name":"type","Data Type":"string","Description":"Entity type (company, person, domain)"},
        {"Column Name":"domain","Data Type":"string","Description":"Source domain"},
        {"Column Name":"company_name","Data Type":"string","Description":"Company name"},
        {"Column Name":"ticker","Data Type":"string","Description":"Stock ticker symbol"},
        {"Column Name":"author","Data Type":"string","Description":"Author name"},
        {"Column Name":"image_url","Data Type":"string","Description":"Image URL"},
        {"Column Name":"url","Data Type":"string","Description":"Source article URL"},
        {"Column Name":"published_at","Data Type":"string","Description":"Published timestamp (raw)"},
        {"Column Name":"title","Data Type":"string","Description":"Article title"},
    ]), use_container_width=True)

# -------------------------
# STEP 3 - Missing values
# -------------------------
st.header("3ï¸âƒ£ Step 3 â€” Missing values (counts & %)")
col1, col2 = st.columns(2)
with col1:
    st.subheader("df_data missing values")
    mv = df_data.isnull().sum()
    mv_pct = (mv / len(df_data) * 100).round(2) if len(df_data) else mv
    mv_df = pd.DataFrame({"missing_count": mv, "missing_pct": mv_pct})
    st.dataframe(mv_df, use_container_width=True)
with col2:
    st.subheader("df_included missing values")
    mv2 = df_included.isnull().sum()
    mv2_pct = (mv2 / len(df_included) * 100).round(2) if len(df_included) else mv2
    mv2_df = pd.DataFrame({"missing_count": mv2, "missing_pct": mv2_pct})
    st.dataframe(mv2_df, use_container_width=True)
log("Displayed missing value summaries")

# -------------------------
# STEP 4 - Standardize date formats (overwrite original columns)
# -------------------------
st.header("4ï¸âƒ£ Step 4 â€” Standardize date formats (Tableau-friendly)")
st.markdown("""
We now standardize all date fields into consistent formats (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS) directly in their existing columns.
No new 'cleaned_*' columns will be created.
""")

date_fields = ["found_at", "published_at", "effective_date"]

progress = st.progress(0)
for i, col in enumerate(date_fields):
    progress.progress((i + 1) / len(date_fields))
    if col in df_data.columns:
        want_date_only = (col == "effective_date")
        df_data[col] = df_data[col].apply(lambda v: make_tableau_friendly_date_str(v, want_date_only=want_date_only))
    if col in df_included.columns:
        want_date_only = (col == "effective_date")
        df_included[col] = df_included[col].apply(lambda v: make_tableau_friendly_date_str(v, want_date_only=want_date_only))
progress.empty()
st.success("Date formats standardized successfully (existing columns updated).")
log("Date columns standardized in-place (no cleaned_* columns created)")

# -------------------------
# STEP 5 - Outlier detection (IQR) for `confidence`
# -------------------------
st.header("5ï¸âƒ£ Step 5 â€” Outlier detection (IQR) for `confidence`")
if "confidence" in df_data.columns:
    df_data["confidence"] = pd.to_numeric(df_data["confidence"], errors="coerce")
    mask_outlier, bounds = detect_outliers_iqr(df_data["confidence"])
    outlier_count = int(mask_outlier.sum()) if len(mask_outlier) else 0
    st.write(f"Outliers detected in `confidence`: {outlier_count}")
    fig1, ax1 = plt.subplots(figsize=(6,2.5))
    sns.boxplot(x=df_data["confidence"], ax=ax1)
    ax1.set_title("Confidence â€” Boxplot")
    st.pyplot(fig1)
    fig2, ax2 = plt.subplots(figsize=(6,2.5))
    sns.histplot(df_data["confidence"].dropna(), bins=30, kde=False, ax=ax2)
    ax2.set_title("Confidence â€” Histogram")
    st.pyplot(fig2)
    st.write("IQR bounds:", bounds)
    log(f"Outlier detection completed (outliers={outlier_count})")
else:
    st.info("No `confidence` column present â€” skipping outlier detection.")
    log("Skipped outlier detection (no confidence column)")

# -------------------------
# STEP 6 - Normality check for `confidence`
# -------------------------
st.header("6ï¸âƒ£ Step 6 â€” Normality check (visual + skew)")
if "confidence" in df_data.columns and df_data["confidence"].dropna().size > 0:
    ser = df_data["confidence"].dropna().astype(float)
    skewness = float(ser.skew())
    kurt = float(ser.kurtosis())
    fig3, ax3 = plt.subplots(figsize=(6,3))
    sns.histplot(ser, bins=30, kde=True, ax=ax3)
    ax3.set_title("Confidence distribution (KDE)")
    st.pyplot(fig3)
    st.write(f"Skewness: {skewness:.3f}  |  Kurtosis: {kurt:.3f}")
    log("Completed normality check for confidence")
else:
    st.info("No numeric `confidence` to check normality.")
    log("Skipped normality check (no numeric confidence)")

# -------------------------
# STEP 7 - Drop duplicates on event_id
# -------------------------
st.header("7ï¸âƒ£ Step 7 â€” Deduplicate by `event_id` (primary key)")
if "event_id" in df_data.columns:
    before_rows = len(df_data)
    dup_count = int(df_data.duplicated(subset=["event_id"]).sum())
    st.write(f"Found duplicate event_id rows: {dup_count}")
    df_data = df_data.drop_duplicates(subset=["event_id"])
    after_rows = len(df_data)
    st.success(f"Dropped duplicates: {before_rows - after_rows} rows removed. New row count: {after_rows}")
    log(f"Dropped {before_rows - after_rows} duplicate rows.")
else:
    st.info("No `event_id` column found; cannot dedupe.")
    log("No event_id present for deduplication")


# -------------------------
# CLEANING & TRANSFORMATION
# -------------------------
st.subheader("Data Cleaning and Transformation")

if df_data is not None and df_included is not None:
    st.write("âœ… Starting cleaning and transformation...")

    # --- Drop unwanted columns ---
    cols_to_drop = ['body', 'title']
    for df_name, df in [('news_events_data', df_data), ('news_events_included', df_included)]:
        existing = [c for c in cols_to_drop if c in df.columns]
        if existing:
            df.drop(columns=existing, inplace=True)
            st.info(f"Dropped columns {existing} from {df_name}")

    # --- Standardize date formats ---
    for col in df_data.columns:
        if "date" in col.lower() or "found_at" in col.lower():
            df_data[col] = pd.to_datetime(df_data[col], errors='coerce').astype(str)

    for col in df_included.columns:
        if "published_at" in col.lower():
            df_included[col] = pd.to_datetime(df_included[col], errors='coerce').astype(str)

    st.success("âœ… Data cleaned and transformed successfully!")

else:
    st.warning("âš ï¸ Please upload both datasets before running this step.")


# -------------------------
# STEP 8 - Null handling & drop large text columns
# -------------------------
st.header("8ï¸âƒ£ Step 8 â€” Null handling & drop large text columns")
if "body" in df_data.columns and drop_body_choice:
    df_data.drop(columns=["body"], inplace=True, errors="ignore")
    st.success("Dropped 'body' column from df_data.")
    log("Dropped body column")
if "title" in df_included.columns:
    # user previously asked to drop title in included; keep drop to reduce payload
    df_included.drop(columns=["title"], inplace=True, errors="ignore")
    st.success("Dropped 'title' column from df_included.")
    log("Dropped title column from df_included")

df_data = nulls_to_blanks(df_data)
df_included = nulls_to_blanks(df_included)
st.info("Replaced NaN/None with blank strings in both datasets.")
log("Replaced nulls with blanks")

# -------------------------
# STEP 9 - Truncate all columns for safe export
# -------------------------
df_data = truncate_all_columns(df_data)
df_included = truncate_all_columns(df_included)
st.success("Truncated long strings to safe lengths for Redshift export.")
log("Truncated columns")

# -------------------------
# STEP 10 - Visual EDA (category + confidence)
# -------------------------
st.header("9ï¸âƒ£ Step 9 â€” Visual EDA: Category & Confidence")
colA, colB = st.columns(2)
with colA:
    st.markdown("### Category distribution")
    if "category" in df_data.columns and not df_data["category"].replace("", np.nan).isnull().all():
        order = df_data["category"].replace("", np.nan).dropna().value_counts().index
        fig4, ax4 = plt.subplots(figsize=(8,5))
        sns.countplot(y="category", data=df_data.replace("", np.nan), order=order, ax=ax4)
        ax4.set_title("Category distribution")
        st.pyplot(fig4)
        st.dataframe(df_data["category"].replace("", np.nan).dropna().value_counts().head(10).reset_index().rename(columns={"index":"Category","category":"Count"}))
    else:
        st.warning("No valid categories to plot.")
with colB:
    st.markdown("### Confidence distribution")
    if "confidence" in df_data.columns:
        conf_valid = pd.to_numeric(df_data["confidence"].replace("", np.nan), errors="coerce").dropna()
        if len(conf_valid) > 0:
            fig5, ax5 = plt.subplots(figsize=(6,4))
            sns.histplot(conf_valid, bins=20, kde=True, ax=ax5)
            ax5.set_title("Confidence histogram")
            st.pyplot(fig5)
            st.markdown(f"**Average confidence:** {round(conf_valid.mean(),4)}")
        else:
            st.warning("No valid numeric confidence values for plotting.")
    else:
        st.info("No 'confidence' column present.")

log("Displayed EDA charts")

# Cleaned data

def upload_cleaned_to_s3(df_data, df_included):
    s3 = boto3.client("s3", region_name="ap-southeast-2")
    bucket_name = "news-events-cleaned-data"

    # Define object keys
    key_data = "cleaned_df_data.csv"
    key_included = "cleaned_df_included.csv"

    # Save locally first
    df_data.to_csv(key_data, index=False)
    df_included.to_csv(key_included, index=False)

    # Upload both files
    with st.spinner("ðŸ“¤ Uploading cleaned files to S3..."):
        s3.upload_file(key_data, bucket_name, key_data)
        s3.upload_file(key_included, bucket_name, key_included)

    st.success("âœ… Cleaned data uploaded successfully to S3.")
    st.info(
        "Lambda will automatically trigger to load data into Redshift.\n"
        f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )

    # Optional: return S3 paths for confirmation/logs
    return {
        "data_s3_path": f"s3://{bucket_name}/{key_data}",
        "included_s3_path": f"s3://{bucket_name}/{key_included}",
    }

# -------------------------
# BEFORE vs AFTER preprocessing (not DQ) â€” Step requested
# -------------------------
st.header("ðŸ” BEFORE vs AFTER preprocessing (shapes & missingness)")
after_snapshot = {
    'df_data_shape': df_data.shape,
    'df_included_shape': df_included.shape,
    'df_data_missing_pct': round(df_data.replace("", np.nan).isnull().mean().mean() * 100, 2) if len(df_data) else 0.0,
    'df_included_missing_pct': round(df_included.replace("", np.nan).isnull().mean().mean() * 100, 2) if len(df_included) else 0.0,
    'df_data_profile_after': basic_profile_df(df_data),
    'df_included_profile_after': basic_profile_df(df_included)
}

col1, col2 = st.columns(2)
with col1:
    st.markdown("### df_data â€” BEFORE vs AFTER")
    st.write("Shape BEFORE:", before_snapshot['df_data_shape'], "â†’ AFTER:", after_snapshot['df_data_shape'])
    st.write("Missing % BEFORE:", before_snapshot['df_data_missing_pct'], "% â†’ AFTER:", after_snapshot['df_data_missing_pct'], "%")
    st.subheader("Profile BEFORE (sample)")
    st.dataframe(before_snapshot['df_data_profile'].head(10), use_container_width=True)
    st.subheader("Profile AFTER (sample)")
    st.dataframe(after_snapshot['df_data_profile_after'].head(10), use_container_width=True)
with col2:
    st.markdown("### df_included â€” BEFORE vs AFTER")
    st.write("Shape BEFORE:", before_snapshot['df_included_shape'], "â†’ AFTER:", after_snapshot['df_included_shape'])
    st.write("Missing % BEFORE:", before_snapshot['df_included_missing_pct'], "% â†’ AFTER:", after_snapshot['df_included_missing_pct'], "%")
    st.subheader("Profile BEFORE (sample)")
    st.dataframe(before_snapshot['df_included_profile'].head(10), use_container_width=True)
    st.subheader("Profile AFTER (sample)")
    st.dataframe(after_snapshot['df_included_profile_after'].head(10), use_container_width=True)

log("Displayed BEFORE vs AFTER preprocessing stats")

st.header("ðŸ’¾ Data Upload & Reporting Operations")

# -------------------------
# Download verified cleaned CSVs
# -------------------------
st.header("Download cleaned CSVs")

import io, csv, pandas as pd

def sanitize_and_truncate(df: pd.DataFrame) -> pd.DataFrame:
    """Cleans, validates, truncates text to 6500 chars, and removes malformed rows."""
    df = df.dropna(how="all")
    df.columns = df.columns.str.strip()

    # Clean text columns
    for col in df.select_dtypes(include=["object"]).columns:
        df[col] = (
            df[col]
            .astype(str)
            .replace({r'\r': ' ', r'\n': ' ', ',': ' ', '"': "'", ';': ' '}, regex=True)
            .str.strip()
            .str.slice(0, 6500)
        )

    # Re-export to check malformed rows (uneven commas)
    buf = io.StringIO()
    df.to_csv(buf, index=False, quoting=csv.QUOTE_ALL, escapechar='\\')
    lines = buf.getvalue().splitlines()
    header_cols = len(df.columns)
    bad_rows = [i for i, line in enumerate(lines[1:], start=2) if line.count(',') + 1 != header_cols]

    if bad_rows:
        st.warning(f"âš ï¸ {len(bad_rows)} malformed rows detected and removed.")
        df = df.drop(df.index[[i - 2 for i in bad_rows]])

    st.info(f"ðŸ§¹ Verified Clean Data: {df.shape[0]} rows Ã— {df.shape[1]} columns")
    return df


# Apply cleaning + validation before providing download buttons
df_data_cleaned = sanitize_and_truncate(df_data)
df_included_cleaned = sanitize_and_truncate(df_included)

buf_d = io.StringIO()
buf_i = io.StringIO()
df_data_cleaned.to_csv(buf_d, index=False, quoting=csv.QUOTE_ALL, escapechar='\\')
df_included_cleaned.to_csv(buf_i, index=False, quoting=csv.QUOTE_ALL, escapechar='\\')

st.download_button("â¬‡ï¸ Download verified df_data (CSV)", data=buf_d.getvalue(),
                   file_name=OUTPUT_DATA_KEY, mime="text/csv")
st.download_button("â¬‡ï¸ Download verified df_included (CSV)", data=buf_i.getvalue(),
                   file_name=OUTPUT_INCLUDED_KEY, mime="text/csv")

log("Provided verified download buttons for cleaned CSVs.")


# -------------------------
# Numeric cleanup for confidence column
# -------------------------
if "confidence" in df_data_cleaned.columns:
    df_data_cleaned["confidence"] = (
        df_data_cleaned["confidence"]
        .replace(["-", "--", "â€”", " ", "null", "NaN", "None"], None)
        .astype(str)
        .str.extract(r"(\d+\.?\d*)")[0]
    )
    df_data_cleaned["confidence"] = pd.to_numeric(df_data_cleaned["confidence"], errors="coerce")


# -------------------------
# Upload verified cleaned CSVs to S3 (Lambda will handle Redshift)
# -------------------------
st.header("Upload cleaned CSVs to S3 (Lambda will push to Redshift)")
st.markdown(f"Files will be uploaded to s3://{CLEANED_BUCKET}/")

def upload_cleaned_csvs_to_s3(df_data, df_included):
    try:
        buf_d = io.StringIO()
        buf_i = io.StringIO()
        df_data.to_csv(buf_d, index=False, quoting=csv.QUOTE_ALL, escapechar='\\')
        df_included.to_csv(buf_i, index=False, quoting=csv.QUOTE_ALL, escapechar='\\')

        s3_client.put_object(Bucket=CLEANED_BUCKET, Key=OUTPUT_DATA_KEY, Body=buf_d.getvalue())
        s3_client.put_object(Bucket=CLEANED_BUCKET, Key=OUTPUT_INCLUDED_KEY, Body=buf_i.getvalue())

        st.success("âœ… Uploaded verified cleaned CSVs to S3 successfully.")
        log(f"Uploaded verified cleaned CSVs to s3://{CLEANED_BUCKET}/{OUTPUT_DATA_KEY} and {OUTPUT_INCLUDED_KEY}")
        st.info("Your Lambda should now pick up these and load into Redshift cleanly.")
    except Exception as e:
        st.error(f"âŒ Failed to upload cleaned CSVs to S3: {e}")
        log(f"Failed S3 upload: {e}")


if st.button("Upload Verified Cleaned CSVs to S3"):
    with st.spinner("Uploading to S3..."):
        upload_cleaned_csvs_to_s3(df_data_cleaned, df_included_cleaned)



# -------------------------
# DQ METRICS: compute & push to public.dq_summary
# -------------------------
st.header("Compute & push DQ metrics (public.dq_summary)")
st.markdown("Click to compute DQ metrics for the cleaned data and insert rows into `public.dq_summary` in Redshift.")

def pct(part, whole):
    try:
        if whole == 0:
            return 0.0
        return float(part) / float(whole) * 100.0
    except Exception:
        return 0.0

def compute_dq_metrics(df: pd.DataFrame, key_columns: list, date_column: str=None):
    n = len(df)
    metrics = []
    freshness_text = None

    # Completeness
    for col in key_columns:
        if col in df.columns:
            non_null = df[col].replace("", np.nan).notnull().sum()
            metrics.append(("Completeness", f"{col}_non_null_pct", round(pct(non_null, n), 2)))
        else:
            metrics.append(("Completeness", f"{col}_non_null_pct", 0.0))

    # Uniqueness (PK = first key)
    if key_columns and key_columns[0] in df.columns:
        pk = key_columns[0]
        unique_count = df[pk].nunique()
        metrics.append(("Uniqueness", f"{pk}_unique_pct", round(pct(unique_count, n), 2)))
    else:
        metrics.append(("Uniqueness", "no_pk_found", 0.0))

    # Accuracy (confidence between 0 and 1)
    if "confidence" in df.columns:
        conf_series = pd.to_numeric(df["confidence"].replace("", np.nan), errors="coerce")
        valid_conf = conf_series.between(0,1).sum()
        metrics.append(("Accuracy", "confidence_valid_pct", round(pct(valid_conf, n), 2)))
    else:
        metrics.append(("Accuracy", "no_confidence_column", 0.0))

    # Consistency (date parseable)
    if date_column:
        cleaned_col = "cleaned_" + date_column if ("cleaned_" + date_column) in df.columns else date_column
        parsed = pd.to_datetime(df[cleaned_col].replace("", pd.NaT), errors="coerce")
        valid_dates = parsed.notnull().sum()
        metrics.append(("Consistency", f"{cleaned_col}_valid_date_pct", round(pct(valid_dates, n), 2)))
    else:
        metrics.append(("Consistency", "no_date_column", 0.0))

    # Timeliness (freshness)
    if date_column and ("cleaned_" + date_column) in df.columns:
        parsed = pd.to_datetime(df["cleaned_" + date_column].replace("", pd.NaT), errors="coerce")
        if parsed.dropna().empty:
            freshness_score = 0.0
            freshness_text = "No valid date values found"
        else:
            max_date = parsed.max()
            now = pd.Timestamp.now()
            diff = relativedelta(now, max_date)
            freshness_text = f"{diff.years} years, {diff.months} months, {diff.days} days old (last record: {max_date.date()})"
            days_delta = (now - max_date).days
            # Freshness %: 100% if last record within 7 days, decays linearly by 100% per week
            freshness_score = max(0.0, 100.0 - (days_delta / 7.0) * 100.0)
            freshness_score = round(min(freshness_score, 100.0), 2)
        metrics.append(("Timeliness", "freshness_score", freshness_score))
    else:
        metrics.append(("Timeliness", "no_date_column", 0.0))

    return metrics, freshness_text

# choose keys (assistant-chosen defaults)
keys_data = ["event_id","summary","category","confidence","found_at","effective_date"]
keys_included = ["included_id","domain","company_name","ticker","url","published_at"]

metrics_data, freshness_data_text = compute_dq_metrics(df_data, keys_data, date_column="effective_date")
metrics_included, freshness_included_text = compute_dq_metrics(df_included, keys_included, date_column="published_at")

def push_metrics_to_redshift(metrics_list, table_label, run_id):
    CREATE_DQ_SQL = """
    CREATE TABLE IF NOT EXISTS public.dq_summary (
        dq_id BIGINT IDENTITY(1,1),
        run_id VARCHAR(100),
        table_name VARCHAR(200),
        dq_dimension VARCHAR(200),
        dq_metric VARCHAR(200),
        dq_value FLOAT,
        dq_status VARCHAR(20),
        dq_check_time TIMESTAMP DEFAULT GETDATE()
    );
    """
    conn = None
    cur = None
    try:
        conn = psycopg2.connect(
            host=REDSHIFT_CONFIG["host"],
            port=REDSHIFT_CONFIG["port"],
            dbname=REDSHIFT_CONFIG["dbname"],
            user=REDSHIFT_CONFIG["user"],
            password=REDSHIFT_CONFIG["password"],
        )
        cur = conn.cursor()
        cur.execute(CREATE_DQ_SQL)
        conn.commit()
        for dim, metric_name, val in metrics_list:
            status = "PASS" if val >= dq_threshold else "FAIL"
            cur.execute(
                """
                INSERT INTO public.dq_summary (run_id, table_name, dq_dimension, dq_metric, dq_value, dq_status)
                VALUES (%s, %s, %s, %s, %s, %s);
                """,
                (run_id, table_label, dim, metric_name, float(val), status)
            )
        conn.commit()
        log(f"Pushed {len(metrics_list)} DQ metric rows for {table_label} to public.dq_summary (run_id={run_id})")
    except Exception as e:
        st.error(f"Failed to push DQ metrics to Redshift: {e}")
        log(f"Failed to push DQ metrics to Redshift: {e}")
    finally:
        if cur: cur.close()
        if conn: conn.close()

def derive_overall(metrics):
    vals = [v for (_, _, v) in metrics if isinstance(v, (int, float))]
    if not vals:
        return 0.0
    return round(sum(vals)/len(vals), 2)

# Button to compute & push DQ metrics
if st.button("Compute & Push DQ metrics to Redshift (public.dq_summary)"):
    run_id = f"run_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    with st.spinner("Computing and pushing DQ metrics..."):
        push_metrics_to_redshift(metrics_data, "news_events_data", run_id)
        push_metrics_to_redshift(metrics_included, "news_events_included", run_id)
    overall1 = derive_overall(metrics_data)
    overall2 = derive_overall(metrics_included)
    overall_combined = round((overall1 + overall2) / 2.0, 2)
    st.success(f"DQ metrics pushed. Overall combined DQ score: {overall_combined}% (threshold: {dq_threshold}%)")
    log(f"Overall combined DQ: {overall_combined}% (data:{overall1} included:{overall2})")
    # SNS alert if below threshold
    if overall_combined < dq_threshold:
        try:
            sns = boto3.client("sns", region_name=AWS_REGION)
            subject = f"âš ï¸ NewsEvents DQ Alert â€” {overall_combined}% < {dq_threshold}%"
            message = (f"Overall combined Data Quality score for news_events_data + news_events_included is {overall_combined:.2f}%\n"
                       f"Threshold: {dq_threshold}%\nRun ID: {run_id}\n\nPlease review public.dq_summary in Redshift for details.")
            sns.publish(TopicArn=SNS_TOPIC_ARN, Subject=subject, Message=message)
            st.warning("Overall DQ below threshold â€” SNS alert published.")
            log("SNS alert published")
        except Exception as e:
            st.error(f"SNS publish failed: {e}")
            log(f"SNS publish failed: {e}")

# Display computed DQ metrics in UI (even before pushing)
st.header("DQ metrics (computed from cleaned data)")
if freshness_data_text:
    st.info(f"news_events_data freshness: {freshness_data_text}")
if freshness_included_text:
    st.info(f"news_events_included freshness: {freshness_included_text}")

st.subheader("news_events_data â€” DQ metrics (per-dimension)")
st.dataframe(pd.DataFrame(metrics_data, columns=["Dimension","Metric","Value"]), use_container_width=True)
st.subheader("news_events_included â€” DQ metrics (per-dimension)")
st.dataframe(pd.DataFrame(metrics_included, columns=["Dimension","Metric","Value"]), use_container_width=True)

overall1 = derive_overall(metrics_data)
overall2 = derive_overall(metrics_included)
overall_combined = round((overall1 + overall2) / 2.0, 2)
st.markdown(f"### ðŸ”” Overall combined DQ score: **{overall_combined}%** (threshold: {dq_threshold}%)")

# -------------------------
# Logs & preview dq_summary
# -------------------------
st.header("Logs & DQ preview")
st.subheader("Application logs (latest)")
st.code("\n".join(reversed(_log_lines[-200:] if _log_lines else ["No logs yet."])))

st.subheader("Recent rows from public.dq_summary (if Redshift accessible)")
try:
    conn = psycopg2.connect(
        host=REDSHIFT_CONFIG["host"],
        port=REDSHIFT_CONFIG["port"],
        dbname=REDSHIFT_CONFIG["dbname"],
        user=REDSHIFT_CONFIG["user"],
        password=REDSHIFT_CONFIG["password"],
    )
    dq_preview = pd.read_sql("SELECT dq_check_time, run_id, table_name, dq_dimension, dq_metric, dq_value, dq_status FROM public.dq_summary ORDER BY dq_check_time DESC LIMIT 50;", conn)
    conn.close()
    st.dataframe(dq_preview, use_container_width=True)
except Exception as e:
    st.info("Could not fetch dq_summary preview from Redshift (connection or permissions issue).")
    log(f"Could not fetch dq_summary preview: {e}")

st.info("Pipeline finished. Use the Download and Upload buttons to provide cleaned CSVs to S3. Click Compute & Push DQ metrics to populate public.dq_summary in Redshift.")